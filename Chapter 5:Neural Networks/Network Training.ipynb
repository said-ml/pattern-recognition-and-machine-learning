{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db603455-0956-4b69-8149-42d444f3dc88",
   "metadata": {},
   "source": [
    " ## 5-2 Network Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988bc725-76a1-4538-aad7-fcee2447cd96",
   "metadata": {},
   "source": [
    "We consider the following error function:\n",
    "\n",
    "$~~~~~~~~~~~~~~~~~~~~~~~$    $ E(w)=\\sum_{n=1}^{N} \\{y(x_{n}, w)-t_{n}\\}^2$\n",
    "\n",
    "our goal is to minimize this function, such there no explicit solution to this problem , so this problem is an $optimization$ problem.\n",
    "\n",
    "one approach widely used is getting the minimal (it may be local) through $gradient$ $~$ $descent$ algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef35af19-4b91-4858-b8d7-c69565843ee1",
   "metadata": {},
   "source": [
    "## 5-2-4 Gradient Descent Optimization:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925d3489-53da-4cb1-9f6c-4af760e05381",
   "metadata": {},
   "source": [
    "$gradient$ $~$ $descent$ is an itertive algorithm for optimization, it $n^{th}$ iteration  giveen by the following expression:\n",
    "\n",
    "$~~~~~~~~~~~~~~~~~~~~~$ $~$ $w^{(n+1)}=w^{(n)}-\\eta \\nabla E(w^{(n)})$\n",
    "\n",
    "where:\n",
    "\n",
    "$\\eta$ is called $ learning$ $~$ $rate$, it represent the pace of the training.\n",
    "\n",
    "$\\nabla E(w^{(n)})$ the derivative of the error function with respect the weights\n",
    "\n",
    "$gradient$ $~$ $descent$ is an itertive algorithm , we need first need to initailize the weights(we prefer to initialize the weights randomly to avoid \n",
    "stabilities of some weights due the symetry...etc)\n",
    "\n",
    "we consider the following example:\n",
    "\n",
    "$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$   $f(w1, w2)=w_{1}^2+w_{1}+1+w_{2}^2+4w_{2}+3$\n",
    "\n",
    "as we can the minimum of this function is $\\hat w=(1, -2)$\n",
    "\n",
    "we have:\n",
    "\n",
    "$~~~~~~~~~~~~~~~~~~~~~~~~~~~~$ $\\nabla f(w1, w2)=[2w_{1}-2, 2w_{2}+4]$\n",
    "\n",
    "we choose:\n",
    "\n",
    "$\\eta=.01 $\n",
    "\n",
    "$w_{0}$ a random vector.\n",
    "\n",
    "number of iteration is 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66f0a7a7-0870-4e33-8ccb-ac160cf485c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9999999995847941, -1.9999999956960288)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient1(w1):\n",
    "    return 2*w1-2\n",
    "\n",
    "def gradient2(w2):\n",
    "    return 2*w2+4\n",
    "w01, w02=np.random.random(), np.random.random()\n",
    "n_iterations=1000\n",
    "learning_rate=.01\n",
    "\n",
    "for __ in range(n_iterations):\n",
    "    w01=w01-learning_rate*gradient1(w01)\n",
    "    w02=w02-learning_rate*gradient2(w02)\n",
    "w=(w01, w02)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44922ef4-dac3-480b-8d9a-1e527c59902b",
   "metadata": {},
   "source": [
    "as you can see $(0.9999999995847941, -1.9999999956960288)\\approx (1, -2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116c5567-c19c-454f-a94b-592e96e702be",
   "metadata": {},
   "source": [
    "## Error Backpropagation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbbe85a-95b4-4daf-9995-2bddd415f6c7",
   "metadata": {},
   "source": [
    "in the real world , when we want to train our $neural$ $~$ $networks$ we deal with multiple layers (more than input and ouput layer), so computing \n",
    "the $gradient$ of the error function rely on the chain formula.\n",
    "\n",
    "let's be $E(w)$ the error function and $y(x_{n}, w)$ our model's prediction, if i want to compute $E(w)$ with respect the weight $w$\n",
    "the chain formula give us:\n",
    "\n",
    "$~~~~~~~~~~~~~~~~~~~~~~~~~~$ $\\dfrac{dE(w)}{dw}=\\dfrac{dE(w)}{dy_{n}}*\\dfrac{dy_{n}}{dw}$\n",
    "\n",
    "we applied this formula repaeditely untill we reatch the input layer.\n",
    "\n",
    "when we obtain the $ gradient$ we applied the gradient descent algorithm.\n",
    "\n",
    "\n",
    "for example check out the FeedForward network functions notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332ff8c9-7099-428a-a4ca-8800121e75a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
